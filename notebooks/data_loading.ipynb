{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5bcc40-d4d2-49f1-85e8-43a2da1d913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itsmeale/code/lab/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "import torchvision as tvis\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b048adf0-fac8-4a1e-b00f-6d744009a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path(\"../data/xray/train/\")\n",
    "classes = os.listdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "681b9730-8fa0-41bf-9c49-4c19ff134243",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classes:\n",
    "    class_path = root_path.joinpath(c)\n",
    "    c_files = [\n",
    "        (c, class_path.joinpath(f))\n",
    "        for f in os.listdir(class_path)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddca1ae1-837e-4b4c-9b68-2e2133620571",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = tvis.transforms.Compose([\n",
    "    tvis.transforms.ToTensor(),\n",
    "    tvis.transforms.Resize((128, 128)),\n",
    "    tvis.transforms.Grayscale(),    \n",
    "])\n",
    "\n",
    "class XRayDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir: str):\n",
    "        self.root_dir = root_dir\n",
    "        self.get_dataset_files()\n",
    "        self.load_dataset()\n",
    "    \n",
    "    def get_dataset_files(self):\n",
    "        root_path = Path(self.root_dir)\n",
    "        classes = os.listdir(root_path)\n",
    "        \n",
    "        c_files = list()\n",
    "        for c in classes:\n",
    "            class_path = root_path.joinpath(c)\n",
    "            c_files += [\n",
    "                (np.float64(classes.index(c)), class_path.joinpath(f))\n",
    "                for f in os.listdir(class_path)\n",
    "            ]\n",
    "\n",
    "        self.files = c_files\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        self.images = [\n",
    "            (\n",
    "                preprocess(Image.open(path)),\n",
    "                torch.as_tensor(c).type(torch.FloatTensor)\n",
    "            )\n",
    "            for c, path in self.files\n",
    "        ]\n",
    "        self.target = [c.item() for _, c in self.images]\n",
    "    \n",
    "    def load_to_gpu(self):\n",
    "        if not torch.cuda.is_available():\n",
    "            raise(\"CUDA isn't available\")\n",
    "        \n",
    "        cuda_images = [\n",
    "            (im.to(\"cuda\"), c.to(\"cuda\"))\n",
    "            for im, c in self.images\n",
    "        ]\n",
    "             \n",
    "        self.images = cuda_images\n",
    "    \n",
    "    def free(self):\n",
    "        for im, c in self.images:\n",
    "            im.to(\"cpu\")\n",
    "            del im\n",
    "            c.to(\"cpu\")\n",
    "            del c\n",
    "        del self.images\n",
    "        del self.target\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # should return a image and a label\n",
    "        return self.images[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # should return the number of images on the dataset\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f93dae8-eee9-42d3-a70d-c502b0ff44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = XRayDataset(\"../data/xray/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf443e2a-b923-4cd4-8728-75ec6069da4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7429064417177914"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd40d53-7470-4991-884d-568cfb98a27f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f628070-651d-4587-8cce-c3e64f393c72",
   "metadata": {},
   "source": [
    "## Modelo generativo de difusÃ£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c144bb1e-28e0-4908-9351-0507cd5a2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16057f4-76c5-4b18-a4f6-c8f7d4afd553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "    dim=16,\n",
    "    dim_mults=(1, 2),\n",
    "    channels=1\n",
    ").cuda()\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size=128,\n",
    "    timesteps=10,\n",
    "    loss_type='l1'\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d9503-a645-47b1-bdbe-23deef21ec38",
   "metadata": {},
   "source": [
    "Imagens normais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd5540c-bfaf-48f6-8e22-37a03b0f9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = list()\n",
    "for i, c in iter(dataset):\n",
    "    if c.item() == 0:\n",
    "        normal.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea16e30d-4370-4bfa-8117-c79e845b1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = torch.stack(normal[:100]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9a8230-7fd1-452a-8f1a-eeed66989d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac51442e-54fe-449c-b708-5db73b1dd078",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 25.00 GiB (GPU 0; 11.77 GiB total capacity; 9.32 GiB already allocated; 1.71 GiB free; 9.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:738\u001b[0m, in \u001b[0;36mGaussianDiffusion.forward\u001b[0;34m(self, img, *args, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, (b,), device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    737\u001b[0m img \u001b[38;5;241m=\u001b[39m normalize_to_neg_one_to_one(img)\n\u001b[0;32m--> 738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:714\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_losses\u001b[0;34m(self, x_start, t, noise)\u001b[0m\n\u001b[1;32m    710\u001b[0m         x_self_cond\u001b[38;5;241m.\u001b[39mdetach_()\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# predict and take gradient step\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_self_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_noise\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    717\u001b[0m     target \u001b[38;5;241m=\u001b[39m noise\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:378\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, time, x_self_cond)\u001b[0m\n\u001b[1;32m    375\u001b[0m     x \u001b[38;5;241m=\u001b[39m downsample(x)\n\u001b[1;32m    377\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block1(x, t)\n\u001b[0;32m--> 378\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block2(x, t)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block1, block2, attn, upsample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mups:\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:82\u001b[0m, in \u001b[0;36mResidual.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:130\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    129\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:258\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    254\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb (h c) x y -> b h c (x y)\u001b[39m\u001b[38;5;124m'\u001b[39m, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads), qkv)\n\u001b[1;32m    256\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m--> 258\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb h d i, b h d j -> b h i j\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m attn \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39msoftmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    260\u001b[0m out \u001b[38;5;241m=\u001b[39m einsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h i j, b h d j -> b h i d\u001b[39m\u001b[38;5;124m'\u001b[39m, attn, v)\n",
      "File \u001b[0;32m~/code/lab/venv/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 25.00 GiB (GPU 0; 11.77 GiB total capacity; 9.32 GiB already allocated; 1.71 GiB free; 9.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    loss = diffusion(normal)\n",
    "    loss.backward()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81031927-8eea-4e2e-854a-3f8adeaf9349",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58d6bd-37a6-4700-ab0b-fb7e4d8888f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rede convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "353e3069-9efe-4050-91a9-a891c8adafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c07365ff-311a-4787-bd2b-896d109d90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bnfc = nn.BatchNorm1d(10)\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(10, stride=2)\n",
    "\n",
    "        self.fc = nn.Linear(3136, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def conv_layer(self, x):\n",
    "        x = self.pool(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "    \n",
    "    def fc_layer(self, x):\n",
    "        x = self.sig((self.fc(x)))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "521ea4bc-718c-43c8-8b40-f8a2852361a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader, optimizer, loss_fn):\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        pred = model(images)\n",
    "        loss = loss_fn(pred, labels.unsqueeze(1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b5ee6c1-6c7c-475a-b3d6-12b826b2f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6fd74fd8-be31-4c70-97b4-a7abcc085770",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load_to_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ec2aea7-3e5c-4c99-96af-1092bf1484b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20dd0c37-2af7-4f28-993e-36136b17c244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.2825\n",
      "Epoch 1, loss: 0.2530\n",
      "Epoch 2, loss: 0.1995\n",
      "Epoch 3, loss: 0.2401\n",
      "Epoch 4, loss: 0.2487\n",
      "Epoch 5, loss: 0.2240\n",
      "Epoch 6, loss: 0.2921\n",
      "Epoch 7, loss: 0.2914\n",
      "Epoch 8, loss: 0.1939\n",
      "Epoch 9, loss: 0.2437\n",
      "Epoch 10, loss: 0.2200\n",
      "Epoch 11, loss: 0.2596\n",
      "Epoch 12, loss: 0.2432\n",
      "Epoch 13, loss: 0.2729\n",
      "Epoch 14, loss: 0.1881\n",
      "Epoch 15, loss: 0.3608\n",
      "Epoch 16, loss: 0.2137\n",
      "Epoch 17, loss: 0.1794\n",
      "Epoch 18, loss: 0.2443\n",
      "Epoch 19, loss: 0.2217\n",
      "Epoch 20, loss: 0.3179\n",
      "Epoch 21, loss: 0.2074\n",
      "Epoch 22, loss: 0.2009\n",
      "Epoch 23, loss: 0.1976\n",
      "Epoch 24, loss: 0.1964\n",
      "Epoch 25, loss: 0.2014\n",
      "Epoch 26, loss: 0.1648\n",
      "Epoch 27, loss: 0.2509\n",
      "Epoch 28, loss: 0.1659\n",
      "Epoch 29, loss: 0.2662\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=.9)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = train_loop(model, train_loader, optimizer, loss_fn)\n",
    "    print(f\"Epoch {epoch}, loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb3d95-0662-4835-8f5d-d125fa597254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
